\documentclass[10pt]{article}

\usepackage{henrian-basic}
\usepackage{henrian-homework}

\newcommand{\rinline}[1]{SOMETHING WRONG WITH KNITR}
%% begin.rcode setup, include=FALSE
opts_chunk$set(fig.path='hw03-fig/latex-', cache.path='hw03-cache/latex-', echo=FALSE,
  fig.align='center', fig.width=6, fig.height=5, out.width='.8\\linewidth')#$
fmtmean = function(xs) { sprintf('%.3f', mean(xs)) }
%% end.rcode

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{cancel}
\usepackage{float}
\usepackage{booktabs}
\usepackage[bottom]{footmisc}

\usepackage{beramono}
\usepackage{listings}
\lstset{basicstyle=\ttfamily}

\usepackage{natbib}
\bibliographystyle{plainnat}
\bibpunct{(}{)}{,}{a}{,}{,}

\makeHeaders{NLP: Homework 3}

\begin{document}

\section{Active Learning}


Before initial incrementing was fixed:
Iteration, Training words, Sample selection function, PCFG F1 score
1, 1561, random, 0.49576
2, 1603, random, 0.41039
3, 1595, random, 0.40158
4, 1442, random, 0.40347
5, 1650, random, 0.38817
6, 1561, random, 0.38799
7, 1513, random, 0.39092
8, 1538, random, 0.39804
9, 1615, random, 0.39944
10, 1568, random, 0.39976
11, 1617, random, 0.39420
12, 1581, random, 0.39490
13, 1350, random, 0.39540
14, 1523, random, 0.39905
15, 1425, random, 0.40239
16, 1626, random, 0.39900
17, 1454, random, 0.39260
18, 1352, random, 0.38055
19, 1449, random, 0.38952
20, 1599, random, 0.37036

================================================================================
Iteration, Training words, Sample selection function, PCFG F1 score
1, 1639, random, 0.49576
2, 1669, random, 0.42029
3, 1537, random, 0.42013
4, 1584, random, 0.40569
5, 1513, random, 0.40685
6, 1469, random, 0.40415
7, 1372, random, 0.40619
8, 1549, random, 0.40579
9, 1600, random, 0.40712
10, 1536, random, 0.40647
11, 1521, random, 0.40634
12, 1495, random, 0.40660
13, 1420, random, 0.40845
14, 1458, random, 0.40583
15, 1467, random, 0.40924
16, 1577, random, 0.40867
17, 1660, random, 0.40885
18, 1460, random, 0.41005
19, 1603, random, 0.41009
20, 1596, random, 0.41038



Iteration, Active training added words, Total training words, Sample selection function, PCFG F1 score
1, 1529, 2777, random, 0.49576
2, 1389, 4166, random, 0.42035
3, 1511, 5677, random, 0.42641
4, 1582, 7259, random, 0.41862
5, 1441, 8700, random, 0.41417
6, 1462, 10162, random, 0.41827
7, 1599, 11761, random, 0.41846
8, 1361, 13122, random, 0.41889
9, 1367, 14489, random, 0.41950
10, 1421, 15910, random, 0.41800
11, 1473, 17383, random, 0.41894
12, 1552, 18935, random, 0.41847
13, 1614, 20549, random, 0.41455
14, 1478, 22027, random, 0.41612
15, 1535, 23562, random, 0.41531
16, 1612, 25174, random, 0.41531
17, 1555, 26729, random, 0.41563
18, 1507, 28236, random, 0.41449
19, 1450, 29686, random, 0.41967
20, 1577, 31263, random, 0.41915
21, 1598, 32861, random, 0.41978
22, 1517, 34378, random, 0.41898
23, 1563, 35941, random, 0.41831
24, 1456, 37397, random, 0.42137
25, 1376, 38773, random, 0.42086
26, 1491, 40264, random, 0.42190
27, 1392, 41656, random, 0.42295
28, 1587, 43243, random, 0.42208
29, 1759, 45002, random, 0.42395
30, 1501, 46503, random, 0.42382
31, 1521, 48024, random, 0.42453
32, 1510, 49534, random, 0.42426
33, 1385, 50919, random, 0.42605
34, 1512, 52431, random, 0.42562
35, 1600, 54031, random, 0.42516
36, 1456, 55487, random, 0.42689
37, 1612, 57099, random, 0.42621
38, 1675, 58774, random, 0.42653
39, 1649, 60423, random, 0.42646
40, 1543, 61966, random, 0.42700
41, 1707, 63673, random, 0.42786
42, 1415, 65088, random, 0.42782
43, 1466, 66554, random, 0.42718
44, 1575, 68129, random, 0.42770
45, 1690, 69819, random, 0.42824
46, 1540, 71359, random, 0.42861
47, 1450, 72809, random, 0.42837
48, 1590, 74399, random, 0.42854
49, 1342, 75741, random, 0.42860
50, 1457, 77198, random, 0.42829

Iteration, Added training words, Total training words, Sample selection method, PCFG F1 score
1, 4028, 5276, length, 0.47028
2, 3248, 8524, length, 0.47164
3, 3031, 11555, length, 0.46383
4, 2869, 14424, length, 0.46464
5, 2744, 17168, length, 0.46758
6, 2654, 19822, length, 0.47008
7, 2548, 22370, length, 0.46501
8, 2458, 24828, length, 0.46550
9, 2385, 27213, length, 0.46453
10, 2334, 29547, length, 0.46573
11, 2280, 31827, length, 0.46585
12, 2231, 34058, length, 0.46560
13, 2202, 36260, length, 0.46390
14, 2160, 38420, length, 0.46354
15, 2118, 40538, length, 0.46290
16, 2092, 42630, length, 0.46348
17, 2040, 44670, length, 0.46285
18, 2011, 46681, length, 0.46317
19, 1980, 48661, length, 0.46405
20, 1939, 50600, length, 0.46432
21, 1909, 52509, length, 0.46369
22, 1860, 54369, length, 0.46478
23, 1839, 56208, length, 0.46657
24, 1800, 58008, length, 0.46696
25, 1794, 59802, length, 0.46678
26, 1740, 61542, length, 0.46650
27, 1740, 63282, length, 0.46643
28, 1683, 64965, length, 0.46671
29, 1680, 66645, length, 0.46677
30, 1648, 68293, length, 0.46535
31, 1620, 69913, length, 0.46605
32, 1620, 71533, length, 0.46662
33, 1568, 73101, length, 0.46571
34, 1560, 74661, length, 0.46639
35, 1559, 76220, length, 0.46767
36, 1500, 77720, length, 0.46793
37, 1500, 79220, length, 0.46725
38, 1463, 80683, length, 0.46805
39, 1440, 82123, length, 0.46782
40, 1427, 83550, length, 0.46770
41, 1380, 84930, length, 0.46757
42, 1380, 86310, length, 0.46772
43, 1350, 87660, length, 0.46674
44, 1320, 88980, length, 0.46704
45, 1320, 90300, length, 0.46720
46, 1270, 91570, length, 0.46788
47, 1260, 92830, length, 0.46827
48, 1242, 94072, length, 0.46860
49, 1200, 95272, length, 0.46904
50, 1200, 96472, length, 0.47020

Iteration, Added training words, Total training words, Sample selection method, PCFG F1 score
1, 1480, 2728, top, 0.40509
2, 1775, 4503, top, 0.40659
3, 1445, 5948, top, 0.39062
4, 1375, 7323, top, 0.39688
5, 1425, 8748, top, 0.39668
6, 1662, 10410, top, 0.39541
7, 1789, 12199, top, 0.39398
8, 1561, 13760, top, 0.39029
9, 1389, 15149, top, 0.38904
10, 1371, 16520, top, 0.39311
11, 1603, 18123, top, 0.39468
12, 1499, 19622, top, 0.39387
13, 1580, 21202, top, 0.39332
14, 1809, 23011, top, 0.39422
15, 1282, 24293, top, 0.39349
16, 1477, 25770, top, 0.39344
17, 1573, 27343, top, 0.39450
18, 1658, 29001, top, 0.39600
19, 1741, 30742, top, 0.39484
20, 1787, 32529, top, 0.39375
21, 1659, 34188, top, 0.39353
22, 1403, 35591, top, 0.39371
23, 1441, 37032, top, 0.39320
24, 1579, 38611, top, 0.39321
25, 1394, 40005, top, 0.39273
26, 1469, 41474, top, 0.39269
27, 1544, 43018, top, 0.39250
28, 1376, 44394, top, 0.39229
29, 1523, 45917, top, 0.39246
30, 1458, 47375, top, 0.39260
31, 1466, 48841, top, 0.39267
32, 1266, 50107, top, 0.39244
33, 1448, 51555, top, 0.39414
34, 1745, 53300, top, 0.39522
35, 1711, 55011, top, 0.39521
36, 1547, 56558, top, 0.39569
37, 1633, 58191, top, 0.39350
38, 1324, 59515, top, 0.39317
39, 1668, 61183, top, 0.39330
40, 1596, 62779, top, 0.39659
41, 1439, 64218, top, 0.39616
42, 1548, 65766, top, 0.39608
43, 1311, 67077, top, 0.39648
44, 1690, 68767, top, 0.39565
45, 1599, 70366, top, 0.39592
46, 1565, 71931, top, 0.39592
47, 1394, 73325, top, 0.39613
48, 1603, 74928, top, 0.39661
49, 1676, 76604, top, 0.39677
50, 1460, 78064, top, 0.39748



Iteration, Added training words, Total training words, Sample selection method, PCFG F1 score
1, 3832, 5080, entropy, 0.47064
2, 3200, 8280, entropy, 0.47277
3, 2963, 11243, entropy, 0.46113
4, 2847, 14090, entropy, 0.46623
5, 2767, 16857, entropy, 0.46896
6, 2604, 19461, entropy, 0.47146
7, 2521, 21982, entropy, 0.47286
8, 2399, 24381, entropy, 0.47058
9, 2353, 26734, entropy, 0.46991
10, 2309, 29043, entropy, 0.46995
11, 2263, 31306, entropy, 0.47312
12, 2250, 33556, entropy, 0.46910
13, 1768, 35324, entropy, 0.47042
14, 2115, 37439, entropy, 0.46948
15, 2063, 39502, entropy, 0.46967
16, 2005, 41507, entropy, 0.47151
17, 1980, 43487, entropy, 0.47064
18, 2001, 45488, entropy, 0.47140
19, 1870, 47358, entropy, 0.47172
20, 1920, 49278, entropy, 0.47143
21, 1780, 51058, entropy, 0.47157
22, 1846, 52904, entropy, 0.47332
23, 1780, 54684, entropy, 0.47388
24, 1799, 56483, entropy, 0.47432
25, 1726, 58209, entropy, 0.47432
26, 1721, 59930, entropy, 0.47520
27, 1679, 61609, entropy, 0.47646
28, 1604, 63213, entropy, 0.47636
29, 1717, 64930, entropy, 0.47577
30, 1620, 66550, entropy, 0.47580
31, 1611, 68161, entropy, 0.47584
32, 1569, 69730, entropy, 0.47638
33, 1435, 71165, entropy, 0.47577
34, 1514, 72679, entropy, 0.47632
35, 1532, 74211, entropy, 0.47649
36, 1498, 75709, entropy, 0.47690
37, 1483, 77192, entropy, 0.47720
38, 1456, 78648, entropy, 0.47774
39, 1367, 80015, entropy, 0.47804
40, 1393, 81408, entropy, 0.47732
41, 1425, 82833, entropy, 0.47733
42, 1397, 84230, entropy, 0.47770
43, 1371, 85601, entropy, 0.47821
44, 1353, 86954, entropy, 0.47847
45, 1306, 88260, entropy, 0.47813
46, 1290, 89550, entropy, 0.47837
47, 1252, 90802, entropy, 0.47857
48, 1227, 92029, entropy, 0.47850
49, 1213, 93242, entropy, 0.47855
50, 1191, 94433, entropy, 0.47976

\subsection{Instructions}

All code and full SBT project, is available at \url{https://github.com/chbrown/nlp.git}. Commands used to run the code can be found in \texttt{hw03.README}.

\end{document}

[4] Using the ParserDemo.java class as a example, develop a simple command line interface to the LexicalizedParser that includes support for active learning. Your package should train a parser on a given training set and evaluate it on a given test set, as with the bundled LexicalizedParser. Additionally, choose a random set of sentences from the "unlabeled" training pool whose word count totals approximately 1500 (this represents approximately 60 additional sentences of average length). Output the original training set plus the annotated versions of the randomly selected sentences as your next training set. Output the remaining "unlabeled" training instances as your next "unlabeled" training pool. Lastly, collect your results for this iteration, including at a minimum the following:

      Iteration number
      Number of training words
      The sample selection function
      PCFG F1 score

[5] Execute 10-20 iterations of your parser for the random selection function, selecting approx 1500 words of additional training data each iteration. You may wish to write a simple test harness script that automates this for you. The random selection function represents a baseline that your more sophisticated sample selection functions should outperform.

[6] Implement the three selection functions describe above (sentence length, normalized probability of the top parse, and tree entropy using the top 10-20 PCFG parses), use each of them to replace random selection in the previous run, and collect results for each. Make sure to collect enough data to plot a learning curve (F1 score versus number of training words) for each sample selection function. Note: if you are sorting by a function that is difficult or costly to compute (such as those requiring parsing of sentences), you may want to compute its value for each instance and cache those values to avoid costly re-parsing.

[7] (Optional) If you have additional time, you may wish to experiment by changing the batch size, initial training set size, or the size of the unlabeled training set.

[Report]

Your hard-copy report (of approximately 4-6 pages) should contain a concise but detailed discussion of the experiments you ran, including nicely formatted learning curves presenting the results. In your discussion, be sure to address at least the following questions:

    Do the active learning methods perform better than random selection of training examples? Why?
    Does active learning help across the complete learning curve, or are there parts of the learning curve where it performs best? Why?
    How do the different methods for measuring uncertainty perform compared to each other? Which ones seem to work best? Try to explain any observed differences between methods.
    How do your results compare to those presented by Hwa (2000)?
